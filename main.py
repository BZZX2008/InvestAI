#!/usr/bin/env python3

import os
import sys
import time
import hashlib
from datetime import datetime, timedelta
from loguru import logger
from data.connectors.local_news_loader import LocalNewsLoader
from core.agents.event_extractor import EventExtractionAgent
from core.agents.coordinator import CoordinatorAgent
from output.report_generator import MarkdownReportGenerator

# å¯¼å…¥æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
from core.cache.smart_cache import llm_cache, data_cache
from core.processing.async_engine import async_engine
from core.processing.priority_system import priority_system
from core.quality.quality_assurance import quality_assurance
from core.monitoring.performance_monitor import performance_monitor

# é…ç½®æ—¥å¿—
logger.add("logs/app.log", rotation="50 MB", level="INFO")


class OptimizedInvestmentSystem:
    def __init__(self):
        self.news_loader = LocalNewsLoader()
        self.event_extractor = EventExtractionAgent()
        self.coordinator = CoordinatorAgent()
        self.report_generator = MarkdownReportGenerator()

        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs("logs", exist_ok=True)
        os.makedirs("data/cache", exist_ok=True)
        os.makedirs("data/chroma_db", exist_ok=True)
        os.makedirs("data/news", exist_ok=True)
        os.makedirs("output/reports", exist_ok=True)

        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        performance_monitor.start_monitoring(interval=30)

        logger.info("ä¼˜åŒ–ç‰ˆæŠ•èµ„åˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def run(self, size: int = 200, use_cache: bool = True):
        """è¿è¡Œä¼˜åŒ–ç‰ˆæŠ•èµ„åˆ†æç³»ç»Ÿ"""
        logger.info("å¼€å§‹æ‰§è¡Œä¼˜åŒ–ç‰ˆæŠ•èµ„åˆ†ææµç¨‹")
        start_time = time.time()

        try:
            # 1. æ‰¹é‡åŠ è½½æ–°é—»æ•°æ®ï¼ˆæ”¯æŒä¸Šä¸‡æ¡æ•°æ®ï¼‰
            logger.info("æ‰¹é‡åŠ è½½æ–°é—»æ•°æ®...")

            #æ³¨æ„ä¸æ˜¯è·å–æ•´ä¸ªæ–‡æ¡£ï¼Œè€Œæ˜¯æŒ‰ç…§sizeåœ¨æ•´ä¸ªæ–‡æ¡£ä¸­è·å–å†…å®¹ï¼Œ ç„¶åå†…å®¹å’ŒidåŒé‡å“ˆå¸Œå»é‡ï¼Œ ç¡®ä¿å”¯ä¸€ã€‚
            news_items = self._load_news_batch(size, use_cache)

            if not news_items:
                logger.warning("æ²¡æœ‰è·å–åˆ°æ–°é—»æ•°æ®ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®")
                self._create_sample_news()
                news_items = self._load_news_batch(size, use_cache)

            performance_monitor.record_processing_time("news_loading", time.time() - start_time)
            data_loading_time = time.time()


            # 2. æ‰¹é‡äº‹ä»¶æå–ä¸åˆ†çº§
            logger.info(f"æ‰¹é‡æå–é‡è¦äº‹ä»¶ï¼Œæ‰¹æ¬¡å¤§å°: {size}...")
            events_data = self._batch_extract_events(news_items, batch_size=50)

            # ä¼˜å…ˆçº§æ’åº
            prioritized_events = priority_system.prioritize_events(
                events_data.get("extracted_events", [])
            )
            events_data["extracted_events"] = prioritized_events

            # è´¨é‡éªŒè¯
            is_valid, errors = quality_assurance.validate_event_extraction(
                events_data.get("extracted_events", [])
            )
            if not is_valid:
                logger.warning(f"äº‹ä»¶æå–è´¨é‡éªŒè¯å¤±è´¥: {errors}")

            performance_monitor.record_processing_time("event_extraction", time.time() - data_loading_time)
            extraction_time = time.time()

            # 3. åè°ƒåˆ†æï¼ˆä½¿ç”¨å¼‚æ­¥å¤„ç†ï¼‰
            logger.info("åè°ƒåˆ†æä»»åŠ¡...")
            coordination_result = self.coordinator.process(events_data)

            # è´¨é‡éªŒè¯
            is_valid, errors = quality_assurance.validate_analysis_output(
                coordination_result.get("analysis_results", {})
            )
            if not is_valid:
                logger.warning(f"åˆ†æè¾“å‡ºè´¨é‡éªŒè¯å¤±è´¥: {errors}")

            performance_monitor.record_processing_time("coordination_analysis", time.time() - extraction_time)
            coordination_time = time.time()

            # 4. ç”ŸæˆæŠ¥å‘Š
            logger.info("ç”ŸæˆæŠ•èµ„æŠ¥å‘Š...")
            report_data = {**events_data, **coordination_result}
            report = self.report_generator.generate_report(report_data)

            performance_monitor.record_processing_time("report_generation", time.time() - coordination_time)

            # 5. ä¿å­˜æŠ¥å‘Šå’Œæ€§èƒ½æ•°æ®
            self._save_report(report)
            self._save_performance_data()

            total_time = time.time() - start_time
            performance_monitor.record_processing_time("total_processing", total_time)

            logger.info(f"æŠ•èµ„åˆ†ææµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

        except Exception as e:
            logger.error(f"ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}")
            performance_monitor.record_processing_time("error_handling", time.time() - start_time)
            raise



    ##æ³¨æ„ä¸æ˜¯è·å–æ•´ä¸ªæ–‡æ¡£ï¼Œè€Œæ˜¯æŒ‰ç…§sizeæ•°é‡åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­è·å–å†…å®¹ï¼Œ ç„¶åå†…å®¹å’ŒidåŒé‡å“ˆå¸Œå»é‡ï¼Œ ç¡®ä¿å”¯ä¸€ã€‚
    def _load_news_batch(self, size: int, use_cache: bool = True) -> list:
        """æ‰¹é‡åŠ è½½æ–°é—»æ•°æ®ï¼Œæ”¯æŒç¼“å­˜å’Œå»é‡"""
        cache_key = f"news_batch_{size}_{self._get_news_cache_key()}"

        if use_cache:
            cached_news = data_cache.get(cache_key)
            if cached_news:
                logger.info(f"ä½¿ç”¨ç¼“å­˜æ–°é—»æ•°æ®ï¼Œæ•°é‡: {len(cached_news)}")
                return cached_news

        # ä½¿ç”¨ç”Ÿæˆå™¨åŠ è½½å¤§é‡æ•°æ®
        all_news = []
        news_count = 0
        duplicate_count = 0

        # è®°å½•å·²å¤„ç†çš„æ–°é—»IDå’Œå†…å®¹å“ˆå¸Œï¼Œç”¨äºå»é‡
        processed_ids = set()
        processed_hashes = set()

        logger.info("å¼€å§‹åŠ è½½æ–°é—»æ•°æ®...")

        """ç”Ÿæˆå™¨æ–¹å¼æ‰¹é‡åŠ è½½æ–°é—»æ•°æ®ï¼ŒèŠ‚çœå†…å­˜"""

        #for news in self.news_loader.load_news_generator(): # ç”¨æœ€æ–°æ–‡æ¡£
        for news in self.news_loader.load_news_generator('news.txt'): #ç”¨æŒ‡å®šæ–‡æ¡£
            news_count += 1

            # å»é‡é€»è¾‘
            news_id = news.get('id')
            #ç”Ÿæˆå†…å®¹å“ˆå¸Œ
            content_hash = self._get_content_hash(news.get('content', ''))

            if news_id and news_id in processed_ids:
                duplicate_count += 1
                continue

            if content_hash in processed_hashes:
                duplicate_count += 1
                continue

            # æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
            all_news.append(news)
            if news_id:
                processed_ids.add(news_id)
            processed_hashes.add(content_hash)

            # å¦‚æœè¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œåœæ­¢åŠ è½½ï¼Œ ä¸æ˜¯è·å–æ•´ä¸ªæ–‡æ¡£
            if len(all_news) >= size:
                break

            # è¿›åº¦æ˜¾ç¤º
            if news_count % 1000 == 0:
                logger.info(f"å·²æ‰«æ {news_count} æ¡æ–°é—»ï¼Œå»é‡å {len(all_news)} æ¡")

        logger.info(f"æ–°é—»åŠ è½½å®Œæˆ: æ‰«æ{news_count}æ¡ï¼Œå»é‡å{len(all_news)}æ¡ï¼Œé‡å¤{duplicate_count}æ¡")

        # ç¼“å­˜ç»“æœ
        if use_cache and all_news:
            data_cache.set(cache_key, all_news, ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜

        return all_news


    def _get_news_cache_key(self) -> str:
        """ç”Ÿæˆæ–°é—»ç¼“å­˜é”®ï¼ŒåŸºäºæ–‡ä»¶ä¿®æ”¹æ—¶é—´"""
        try:
            news_dir = "data/news"
            if not os.path.exists(news_dir):
                return "default"

            # è·å–æœ€æ–°æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            latest_mtime = 0

            for filename in os.listdir(news_dir):
                if filename.endswith('.txt'):
                    filepath = os.path.join(news_dir, filename)
                    mtime = os.path.getmtime(filepath)
                    latest_mtime = max(latest_mtime, mtime)

            return str(int(latest_mtime))
        except:
            return "default"


    #æ ¹æ®å†…å®¹ç”Ÿæˆå“ˆå¸Œ
    def _get_content_hash(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        return hashlib.md5(content.encode()).hexdigest()


    #å°†æ€»sizeï¼Œåˆ†æ‰¹æ¬¡äº¤ä¸ªå¤§æ¨¡å‹å¤„ç†ï¼Œè§£å†³ä¸Šä¸‹æ–‡é•¿åº¦é—®é¢˜ï¼Œæœ€ååˆå¹¶å„æ‰¹æ¬¡çš„å¤„ç†å†…å®¹
    def _batch_extract_events(self, news_items: list, batch_size: int = 50) -> dict:
        """æ‰¹é‡æå–äº‹ä»¶"""
        if not news_items:
            return {"extracted_events": [], "total_processed": 0}

        logger.info(f"å¼€å§‹æ‰¹é‡äº‹ä»¶æå–ï¼Œæ€»æ–°é—»æ•°: {len(news_items)}")

        # åˆ†æ‰¹å¤„ç†
        all_events = []#æ€»çš„å¤„ç†å†…å®¹
        total_batches = (len(news_items) + batch_size - 1) // batch_size


        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            end_idx = min((batch_num + 1) * batch_size, len(news_items))
            batch_news = news_items[start_idx:end_idx]

            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}, æ–°é—»æ•°: {len(batch_news)}")

            # æ‰¹é‡å¤„ç†å•ä¸ªæ‰¹æ¬¡, è°ƒç”¨å¤§æ¨¡å‹å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œè§£å†³å¤§æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦é—®é¢˜
            batch_events = self._process_news_batch(batch_news)

            #å•æ‰¹æ¬¡å¤„ç†åå†…å®¹æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
            all_events.extend(batch_events)

            # è¿›åº¦æ˜¾ç¤º
            if (batch_num + 1) % 5 == 0 or (batch_num + 1) == total_batches:
                logger.info(f"è¿›åº¦: {batch_num + 1}/{total_batches}æ‰¹æ¬¡ï¼Œå·²æå– {len(all_events)} ä¸ªäº‹ä»¶")

        return {
            "extracted_events": all_events,
            "total_processed": len(news_items),
            "batch_info": {
                "batch_size": batch_size,
                "total_batches": total_batches,
                "events_per_news": len(all_events) / len(news_items) if news_items else 0
            }
        }



    def _process_news_batch(self, news_batch: list) -> list:
        """å¤„ç†å•ä¸ªæ–°é—»æ‰¹æ¬¡"""
        if not news_batch:
            return []

        # æ„å»ºæ‰¹é‡æç¤ºè¯
        batch_prompt = self._build_batch_prompt(news_batch)

        try:
            # ä½¿ç”¨LLMæ‰¹é‡å¤„ç†
            response = self.event_extractor.llm_call(batch_prompt, use_cache=True)
         #   print(response)

            # è§£ææ‰¹é‡å“åº”
            batch_events = self._parse_batch_response(response, news_batch)

            # è®°å½•LLMè°ƒç”¨
            performance_monitor.record_llm_call()

            return batch_events

        except Exception as e:
            logger.error(f"æ‰¹é‡äº‹ä»¶æå–å¤±è´¥: {e}")
            # å›é€€åˆ°å•æ¡å¤„ç†
            return self._fallback_single_processing(news_batch)

    def _build_batch_prompt(self, news_batch: list) -> str:
        """æ„å»ºæ‰¹é‡å¤„ç†æç¤ºè¯"""
        news_list_text = ""
        for i, news in enumerate(news_batch):
            content = news.get('content', '')[:800]  # é™åˆ¶å†…å®¹é•¿åº¦
            news_list_text += f"{i + 1}. å†…å®¹: {content}\n"
#"investment_implication": "æŠ•èµ„å«ä¹‰è¯´æ˜",
        prompt = f"""
        è¯·æ‰¹é‡åˆ†æä»¥ä¸‹å¤šæ¡æ–°é—»ï¼Œæå–å¯¹æŠ•èµ„å†³ç­–æœ‰å½±å“çš„äº‹ä»¶ï¼š

        {news_list_text}

        è¯·ä¸ºæ¯æ¡æ–°é—»æå–æŠ•èµ„ç›¸å…³äº‹ä»¶ï¼Œå¹¶æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š

        {{
          "batch_events": [
            {{
              "news_index": 1,
              "core_event": "äº‹ä»¶æè¿°",
              "impact_level": "high/medium/low",
              "time_horizon": "short/mid/long",
              "affected_assets": ["æ ‡çš„1", "æ ‡çš„2"],
              "Label"ï¼š"æ ‡æ³¨æ¶‰åŠï¼šå®è§‚ç»æµæŒ‡æ ‡/æ”¿ç­–å˜åŠ¨/è¡Œä¸šé¢ è¦†äº‹ä»¶/å›½é™…å…³è”äº‹ä»¶/ä¼ä¸šæˆ–å…¬å¸åŠ¨æ€/å¸‚åœºå¼‚åŠ¨ä¸äº¤æ˜“äº‹ä»¶/ç¤¾ä¼šä¸æ°‘ç”Ÿäº‹ä»¶/ç§‘æŠ€ä¸åˆ›æ–°è¿›å±•/æ³•å¾‹ä¸è¯‰è®¼/ç›‘ç®¡è¡ŒåŠ¨ä¸åˆè§„äº‹ä»¶/åœ°ç†ä¸åŒºåŸŸäº‹ä»¶/äººäº‹å˜åŠ¨ï¼ˆéå…¬å¸å±‚é¢ï¼‰/èˆ†æƒ…äº‹ä»¶ç­‰ç­‰"
              "importance ranking": "äº‹ä»¶å½±å“ä¸é‡è¦æ€§è¯„åˆ†(1-5)",
              "basis":"è¯„åˆ†ä¾æ®ä¸åŸç†"
              
            }},
            ...
          ]
        }}

        è¦æ±‚ï¼š
        1. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹
        2. ä¸ºæ¯æ¡æ–°é—»è‡³å°‘æå–ä¸€ä¸ªäº‹ä»¶
        3. é‡ç‚¹å…³æ³¨é‡‘èã€ç»æµã€æ”¿ç­–ç›¸å…³äº‹ä»¶
        4. å¦‚æœæ–°é—»æ— å…³æŠ•èµ„ï¼Œimpact_levelè®¾ä¸º"low"
        """
        return prompt

    def _parse_batch_response(self, response: str, original_news: list) -> list:
        """è§£ææ‰¹é‡å“åº”"""
        import json
        import re

        try:
            # æ¸…ç†å“åº”
            cleaned = re.sub(r'```json\s*', '', response)
            cleaned = re.sub(r'```\s*', '', cleaned)
            cleaned = cleaned.strip()

            # æå–JSON
            start = cleaned.find('{')
            end = cleaned.rfind('}') + 1

            if start == -1 or end == 0:
                raise ValueError("æœªæ‰¾åˆ°JSONå†…å®¹")

            json_str = cleaned[start:end]
            result = json.loads(json_str)

            batch_events = result.get('batch_events', [])

            # éªŒè¯äº‹ä»¶æ•°é‡
            if len(batch_events) != len(original_news):
                logger.warning(f"æ‰¹é‡äº‹ä»¶æ•°é‡ä¸åŒ¹é…: æœŸæœ›{len(original_news)}ï¼Œå®é™…{len(batch_events)}")

            # æ·»åŠ åŸå§‹æ–°é—»ä¿¡æ¯
            for event in batch_events:
                news_index = event.get('news_index', 1) - 1
                if 0 <= news_index < len(original_news):
                    original_news_item = original_news[news_index]
                    event['source_file'] = original_news_item.get('source_file')
                    event['source_line'] = original_news_item.get('source_line')
                    event['original_timestamp'] = original_news_item.get('timestamp')
                    event['original_category'] = original_news_item.get('category')

            return batch_events

        except Exception as e:
            logger.error(f"æ‰¹é‡å“åº”è§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response}")
            raise

    def _fallback_single_processing(self, news_batch: list) -> list:
        """å›é€€åˆ°å•æ¡å¤„ç†"""
        logger.info("ä½¿ç”¨å•æ¡å¤„ç†å›é€€æ–¹æ¡ˆ")
        all_events = []

        for news in news_batch:
            try:
                # ä½¿ç”¨ç°æœ‰çš„äº‹ä»¶æå–å™¨å•æ¡å¤„ç†
                events_data = self.event_extractor.process([news])
                events = events_data.get('extracted_events', [])
                all_events.extend(events)

                # è®°å½•LLMè°ƒç”¨
                performance_monitor.record_llm_call()

            except Exception as e:
                logger.error(f"å•æ¡äº‹ä»¶æå–å¤±è´¥: {e}")
                # åˆ›å»ºé»˜è®¤äº‹ä»¶
                default_event = self._create_default_event(news)
                all_events.append(default_event)

        return all_events

    def _create_default_event(self, news: dict) -> dict:
        """åˆ›å»ºé»˜è®¤äº‹ä»¶"""
        return {
            "core_event": news.get('content', '')[:100],
            "impact_level": "low",
            "time_horizon": "short",
            "affected_assets": [],
            "investment_implication": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
            "confidence": 0.3,
            "source_file": news.get('source_file'),
            "source_line": news.get('source_line')
        }

    def _create_sample_news(self):
        """åˆ›å»ºç¤ºä¾‹æ–°é—»æ–‡ä»¶"""
        sample_content = """ID: 4422906 | æ—¶é—´: 2025-10-08 22:46 | æ ‡ç­¾: å…¬å¸/å¸‚åœº | å†…å®¹: ç¾å…‰ç§‘æŠ€å…¬å¸è‚¡ä»·ä¸Šæ¶¨5.1%ï¼Œå—AIèŠ¯ç‰‡éœ€æ±‚æ¨åŠ¨ã€‚
ID: 4422907 | æ—¶é—´: 2025-10-08 22:45 | æ ‡ç­¾: æ”¿ç­–/å®è§‚ | å†…å®¹: ç¾è”å‚¨ä¸»å¸­è¡¨ç¤ºé€šèƒ€å‹åŠ›ç¼“è§£ï¼Œå¯èƒ½è€ƒè™‘é™æ¯ã€‚
ID: 4422908 | æ—¶é—´: 2025-10-08 22:44 | æ ‡ç­¾: è¡Œä¸š/ç§‘æŠ€ | å†…å®¹: äººå·¥æ™ºèƒ½èŠ¯ç‰‡éœ€æ±‚æ¿€å¢ï¼Œè‹±ä¼Ÿè¾¾ä¸šç»©è¶…é¢„æœŸã€‚
ID: 4422909 | æ—¶é—´: 2025-10-08 22:43 | æ ‡ç­¾: å¸‚åœº/å•†å“ | å†…å®¹: å›½é™…æ²¹ä»·çªç ´85ç¾å…ƒï¼Œåœ°ç¼˜æ”¿æ²»ç´§å¼ å±€åŠ¿å‡çº§ã€‚
ID: 4422910 | æ—¶é—´: 2025-10-08 22:42 | æ ‡ç­¾: ç»æµ/æ•°æ® | å†…å®¹: ç¾å›½éå†œå°±ä¸šæ•°æ®è¶…é¢„æœŸï¼Œå¤±ä¸šç‡é™è‡³3.5%ã€‚"""

        sample_file = "data/news/sample_batch_news.txt"
        with open(sample_file, 'w', encoding='utf-8') as f:
            f.write(sample_content)

        logger.info(f"å·²åˆ›å»ºç¤ºä¾‹æ–°é—»æ–‡ä»¶: {sample_file}")

    def _save_report(self, report: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"output/reports/investment_report_{timestamp}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)

        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {filename}")

        # åŒæ—¶åœ¨æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        print("\n" + "=" * 50)
        print("æŠ•èµ„åˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("=" * 50)
        lines = report.split('\n')
        for line in lines[:25]:  # åªè¾“å‡ºå‰25è¡Œ
            if line.strip():
                print(line)
        print("=" * 50)
        print(f"å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {filename}")

    def _save_performance_data(self):
        """ä¿å­˜æ€§èƒ½æ•°æ®"""
        from datetime import datetime
        import json

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        perf_report = performance_monitor.get_performance_report()
        quality_report = quality_assurance.get_quality_report()
        cache_stats = llm_cache.get_stats()

        # ä¿å­˜æ€§èƒ½æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        perf_filename = f"output/reports/performance_{timestamp}.json"

        performance_data = {
            "timestamp": timestamp,
            "performance": perf_report,
            "quality": quality_report,
            "cache": cache_stats
        }

        with open(perf_filename, 'w', encoding='utf-8') as f:
            json.dump(performance_data, f, indent=2, ensure_ascii=False)

        logger.info(f"æ€§èƒ½æ•°æ®å·²ä¿å­˜: {perf_filename}")

        # è¾“å‡ºå…³é”®æ€§èƒ½æŒ‡æ ‡
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ€»è¿è¡Œæ—¶é—´: {perf_report['uptime_human']}")
        print(f"   LLMè°ƒç”¨æ¬¡æ•°: {perf_report['llm_calls_total']}")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {perf_report['cache_performance']['hit_rate']}%")
        print(f"   æ€»ä½“è´¨é‡åˆ†æ•°: {quality_report['overall_quality']:.2f}")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œåœæ­¢æ€§èƒ½ç›‘æ§"""
        performance_monitor.stop_monitoring()


def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='AIæŠ•èµ„åˆ†æç³»ç»Ÿ')
    parser.add_argument('--batch-size', type=int, default=500, help='æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤100æ¡æ–°é—»')
    #parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    #parser.add_argument('--test-mode', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨å°æ‰¹é‡æ•°æ®')

    args = parser.parse_args()

    # æµ‹è¯•æ¨¡å¼ä½¿ç”¨å°æ‰¹æ¬¡
    #if args.test_mode:
     #   args.batch_size = 10
      #  logger.info("æµ‹è¯•æ¨¡å¼å¯ç”¨ï¼Œä½¿ç”¨å°æ‰¹æ¬¡å¤„ç†")

    system = OptimizedInvestmentSystem()

    try:
        system.run(
            batch_size=500,
            #use_cache=not args.no_cache
        )

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()